{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAONE ëŒë ¤ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‹¤í–‰ë°©ë²• \n",
    "\n",
    "- ìœ„ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì­‰ ëŒë ¤ë³´ë©´ ë©ë‹ˆë‹¤. \n",
    "- inferenceì˜ ê²½ìš° Train ë¶€ë¶„ ì§ì „ì— ë©ˆì¶˜ ë‹¤ìŒ ë°”ë¡œ inferenceìª½ìœ¼ë¡œ ê°€ë³´ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "- OOM ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°œìƒí–ˆì„ ê²½ìš° \n",
    "  - ì—¬ëŸ¬ë²ˆ ì¬ì‹œë„ë¥¼ í•˜ë©´ ë˜ëŠ”ë°, ì¼ë‹¨ í„°ë¯¸ë„ì—ì„œ 'ps aux | grep python'ì„ í†µí•´ ì‹¤í–‰ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. \n",
    "  - ê·¸ ë‹¤ìŒ '/opt/conda/bin/python -m ipykernel_launcher' ë¨¸ì‹œê¸°ê°€ ìˆì„ í…ë°, \n",
    "  - root ë°”ë¡œ ì˜†ì— ìˆëŠ” ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì…”ì„œ kill -9 (ì§€ì›Œì•¼ ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ ëª©ë¡ë“¤)ë¡œ ì‹¹ ë‹¤ ì§€ìš´ ë’¤, ë‹¤ì‹œ ëŒë ¤ì•¼ í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig \n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚œìˆ˜ ê³ ì •\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>problems</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-425</td>\n",
       "      <td>ìƒì†Œí•˜ì—¬ ì•„ë¢°ê¸°ë¥¼ , â€œì‹ ì´ ì¢Œì°¸ ì°¬ ì†¡ì¤€ê¸¸ì´ ì˜¬ë¦° ì°¨ìë¥¼ ë³´ì•˜ëŠ”ë° , ìƒë³µ(å–ªæœ)...</td>\n",
       "      <td>{'question': 'ìƒì†Œí•œ ì¸ë¬¼ì´ ì†í•œ ë¶•ë‹¹ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒë§Œì„ ëª¨ë‘...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-426</td>\n",
       "      <td>(ê°€)ì€/ëŠ” ì˜ë³‘ê³„ì—´ê³¼ ì• êµ­ê³„ëª½ ìš´ë™ ê³„ì—´ì˜ ë¹„ë°€ê²°ì‚¬ê°€ ëª¨ì—¬ ê²°ì„±ëœ ì¡°ì§ìœ¼ë¡œ, ì´ì‚¬...</td>\n",
       "      <td>{'question': '(ê°€)ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?', 'choice...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-427</td>\n",
       "      <td>ë‚˜ëŠ” ì‚¼í•œ(ä¸‰éŸ“) ì‚°ì²œì˜ ìŒë•ì„ ì…ì–´ ëŒ€ì—…ì„ ì´ë£¨ì—ˆë‹¤.(ê°€)ëŠ”/ì€ ìˆ˜ë•(æ°´å¾·)ì´ ìˆœ...</td>\n",
       "      <td>{'question': '(ê°€) ì§€ì—­ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?', 'choice...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-428</td>\n",
       "      <td>ì´ ë‚  ì†Œì •ë°©ì´ ë¶€ì´ê´€ ê¹€ì¸ë¬¸ ë“±ê³¼ í•¨ê»˜ ê¸° ë²Œí¬ì— ë„ì°©í•˜ì—¬ ë°±ì œ êµ°ì‚¬ì™€ ë§ˆì£¼ì³¤ë‹¤....</td>\n",
       "      <td>{'question': 'ë°‘ì¤„ ì¹œ â€˜ê·¸â€™ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?', 'choi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-429</td>\n",
       "      <td>ì„ ë¹„ë“¤ ìˆ˜ë§Œ ëª…ì´ ëŒ€ê¶ ì•ì— ëª¨ì—¬ ë§Œ ë™ë¬˜ì™€ ì„œì›ì„ ë‹¤ì‹œ ì„¤ë¦½í•  ê²ƒì„ ì²­í•˜ë‹ˆ, (ê°€...</td>\n",
       "      <td>{'question': '(ê°€) ì¸ë¬¼ì´ ì¶”ì§„í•œ ì •ì±…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?', 'ch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>generation-for-nlp-2893</td>\n",
       "      <td>â€œí—ê°’ì— íŒ”ë¦¬ëŠ” ëƒ‰ë™ ì˜¤ë Œì§€ì£¼ìŠ¤ë§Œ ì”ëœ© ì‚¬ê°€ê³ , ì¿ í°ì„ ì˜¤ë ¤ ëª¨ì•˜ìœ¼ë©°, êµ¬ë© ë‚œ ìŠ¤...</td>\n",
       "      <td>{'question': 'ì­ ë§¥ë„ë„ë“œê°€ ë‚¨ê¸´ ìœ ì‚°ì˜ ì´ì•¡ì€ ì–¼ë§ˆì¸ê°€?', 'choi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>generation-for-nlp-2894</td>\n",
       "      <td>ë„·ê¸°ì–´ì½”ë¦¬ì•„(ì§€ì‚¬ì¥ ê¹€ì§„ê²¸, ì´í•˜ ë„·ê¸°ì–´)ê°€ ë®¤ëŸ´ ìº”ë²„ìŠ¤ë¥¼ ê°€ì§€ê³  ë„·ê¸°ì–´ SNSì—ì„œ...</td>\n",
       "      <td>{'question': \"ë„·ê¸°ì–´ê°€ ëª¨ì§‘í•˜ëŠ” 'ë®¤ëŸ´ ê³µì‹ ë„ìŠ¨íŠ¸'ì˜ ì£¼ìš” ì—­í• ì€ ë¬´ì—‡...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>generation-for-nlp-2895</td>\n",
       "      <td>ì„œìš¸ ì„±ë™êµ¬ ì˜¥ìˆ˜ë™ê³¼ ê¸ˆí˜¸ë™ì€ ë§ë¶™ì–´ ìˆëŠ” ë™ë„¤ì§€ë§Œ ì•„íŒŒíŠ¸ê°’ì€ ê°™ì€ ë©´ì ì—ì„œ 1ì–µì›...</td>\n",
       "      <td>{'question': 'ì˜¥ìˆ˜ë™ê³¼ ê¸ˆí˜¸ë™ì˜ ì•„íŒŒíŠ¸ê°’ ì°¨ì´ê°€ ë°œìƒí•œ ì£¼ëœ ì´ìœ ëŠ” ë¬´ì—‡...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>generation-for-nlp-2896</td>\n",
       "      <td>ë°©í•˜ë‚¨ ê³ ìš©ë…¸ë™ë¶€ ì¥ê´€(ì‚¬ì§„)ì´ ì·¨ì„ í›„ ì²« ì™¸ë¶€ í–‰ì‚¬ë¡œ 5ë…„ ë§Œì— ì¼ìë¦¬ë¥¼ 3.5...</td>\n",
       "      <td>{'question': 'ë°©í•˜ë‚¨ ê³ ìš©ë…¸ë™ë¶€ ì¥ê´€ì´ ë°©ë¬¸í•œ ê¸°ì—…ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>generation-for-nlp-2899</td>\n",
       "      <td>ì§„ì‹¤í•œ ì‚¬ë‘ì€ ê¸°ì¨ê³¼ í‰í™”ë¥¼ ë‚³ê³  ê³ í†µì„ ì¤„ì—¬ì¤€ë‹¤. ê·¸ê²ƒì€ ìì• , ì—°ë¯¼, ê¸°ì¨, í‰...</td>\n",
       "      <td>{'question': 'í‹±ë‚«í•œ ìŠ¤ë‹˜ì´ ê°•ì¡°í•˜ëŠ” ì§„ì‹¤í•œ ì‚¬ë‘ì˜ ìš”ì†Œê°€ ì•„ë‹Œ ê²ƒì€ ë¬´...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2031 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0      generation-for-nlp-425   \n",
       "1      generation-for-nlp-426   \n",
       "2      generation-for-nlp-427   \n",
       "3      generation-for-nlp-428   \n",
       "4      generation-for-nlp-429   \n",
       "...                       ...   \n",
       "2026  generation-for-nlp-2893   \n",
       "2027  generation-for-nlp-2894   \n",
       "2028  generation-for-nlp-2895   \n",
       "2029  generation-for-nlp-2896   \n",
       "2030  generation-for-nlp-2899   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     ìƒì†Œí•˜ì—¬ ì•„ë¢°ê¸°ë¥¼ , â€œì‹ ì´ ì¢Œì°¸ ì°¬ ì†¡ì¤€ê¸¸ì´ ì˜¬ë¦° ì°¨ìë¥¼ ë³´ì•˜ëŠ”ë° , ìƒë³µ(å–ªæœ)...   \n",
       "1     (ê°€)ì€/ëŠ” ì˜ë³‘ê³„ì—´ê³¼ ì• êµ­ê³„ëª½ ìš´ë™ ê³„ì—´ì˜ ë¹„ë°€ê²°ì‚¬ê°€ ëª¨ì—¬ ê²°ì„±ëœ ì¡°ì§ìœ¼ë¡œ, ì´ì‚¬...   \n",
       "2     ë‚˜ëŠ” ì‚¼í•œ(ä¸‰éŸ“) ì‚°ì²œì˜ ìŒë•ì„ ì…ì–´ ëŒ€ì—…ì„ ì´ë£¨ì—ˆë‹¤.(ê°€)ëŠ”/ì€ ìˆ˜ë•(æ°´å¾·)ì´ ìˆœ...   \n",
       "3     ì´ ë‚  ì†Œì •ë°©ì´ ë¶€ì´ê´€ ê¹€ì¸ë¬¸ ë“±ê³¼ í•¨ê»˜ ê¸° ë²Œí¬ì— ë„ì°©í•˜ì—¬ ë°±ì œ êµ°ì‚¬ì™€ ë§ˆì£¼ì³¤ë‹¤....   \n",
       "4     ì„ ë¹„ë“¤ ìˆ˜ë§Œ ëª…ì´ ëŒ€ê¶ ì•ì— ëª¨ì—¬ ë§Œ ë™ë¬˜ì™€ ì„œì›ì„ ë‹¤ì‹œ ì„¤ë¦½í•  ê²ƒì„ ì²­í•˜ë‹ˆ, (ê°€...   \n",
       "...                                                 ...   \n",
       "2026  â€œí—ê°’ì— íŒ”ë¦¬ëŠ” ëƒ‰ë™ ì˜¤ë Œì§€ì£¼ìŠ¤ë§Œ ì”ëœ© ì‚¬ê°€ê³ , ì¿ í°ì„ ì˜¤ë ¤ ëª¨ì•˜ìœ¼ë©°, êµ¬ë© ë‚œ ìŠ¤...   \n",
       "2027  ë„·ê¸°ì–´ì½”ë¦¬ì•„(ì§€ì‚¬ì¥ ê¹€ì§„ê²¸, ì´í•˜ ë„·ê¸°ì–´)ê°€ ë®¤ëŸ´ ìº”ë²„ìŠ¤ë¥¼ ê°€ì§€ê³  ë„·ê¸°ì–´ SNSì—ì„œ...   \n",
       "2028  ì„œìš¸ ì„±ë™êµ¬ ì˜¥ìˆ˜ë™ê³¼ ê¸ˆí˜¸ë™ì€ ë§ë¶™ì–´ ìˆëŠ” ë™ë„¤ì§€ë§Œ ì•„íŒŒíŠ¸ê°’ì€ ê°™ì€ ë©´ì ì—ì„œ 1ì–µì›...   \n",
       "2029  ë°©í•˜ë‚¨ ê³ ìš©ë…¸ë™ë¶€ ì¥ê´€(ì‚¬ì§„)ì´ ì·¨ì„ í›„ ì²« ì™¸ë¶€ í–‰ì‚¬ë¡œ 5ë…„ ë§Œì— ì¼ìë¦¬ë¥¼ 3.5...   \n",
       "2030  ì§„ì‹¤í•œ ì‚¬ë‘ì€ ê¸°ì¨ê³¼ í‰í™”ë¥¼ ë‚³ê³  ê³ í†µì„ ì¤„ì—¬ì¤€ë‹¤. ê·¸ê²ƒì€ ìì• , ì—°ë¯¼, ê¸°ì¨, í‰...   \n",
       "\n",
       "                                               problems  question_plus  \n",
       "0     {'question': 'ìƒì†Œí•œ ì¸ë¬¼ì´ ì†í•œ ë¶•ë‹¹ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒë§Œì„ ëª¨ë‘...            NaN  \n",
       "1     {'question': '(ê°€)ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?', 'choice...            NaN  \n",
       "2     {'question': '(ê°€) ì§€ì—­ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?', 'choice...            NaN  \n",
       "3     {'question': 'ë°‘ì¤„ ì¹œ â€˜ê·¸â€™ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?', 'choi...            NaN  \n",
       "4     {'question': '(ê°€) ì¸ë¬¼ì´ ì¶”ì§„í•œ ì •ì±…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?', 'ch...            NaN  \n",
       "...                                                 ...            ...  \n",
       "2026  {'question': 'ì­ ë§¥ë„ë„ë“œê°€ ë‚¨ê¸´ ìœ ì‚°ì˜ ì´ì•¡ì€ ì–¼ë§ˆì¸ê°€?', 'choi...            NaN  \n",
       "2027  {'question': \"ë„·ê¸°ì–´ê°€ ëª¨ì§‘í•˜ëŠ” 'ë®¤ëŸ´ ê³µì‹ ë„ìŠ¨íŠ¸'ì˜ ì£¼ìš” ì—­í• ì€ ë¬´ì—‡...            NaN  \n",
       "2028  {'question': 'ì˜¥ìˆ˜ë™ê³¼ ê¸ˆí˜¸ë™ì˜ ì•„íŒŒíŠ¸ê°’ ì°¨ì´ê°€ ë°œìƒí•œ ì£¼ëœ ì´ìœ ëŠ” ë¬´ì—‡...            NaN  \n",
       "2029  {'question': 'ë°©í•˜ë‚¨ ê³ ìš©ë…¸ë™ë¶€ ì¥ê´€ì´ ë°©ë¬¸í•œ ê¸°ì—…ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?'...            NaN  \n",
       "2030  {'question': 'í‹±ë‚«í•œ ìŠ¤ë‹˜ì´ ê°•ì¡°í•˜ëŠ” ì§„ì‹¤í•œ ì‚¬ë‘ì˜ ìš”ì†Œê°€ ì•„ë‹Œ ê²ƒì€ ë¬´...            NaN  \n",
       "\n",
       "[2031 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load the train dataset\n",
    "# TODO Train Data ê²½ë¡œ ì…ë ¥\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `assignment_2_persona` has been saved to /data/ephemeral/home/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `assignment_2_persona`\n"
     ]
    }
   ],
   "source": [
    "# ë³¸ì¸ì˜ Huggingface auth token ì…ë ¥\n",
    "## Jupyter labì—ì„œ ë¡œê·¸ì¸ í•˜ëŠ” textboxê°€ ë‚˜ì˜¤ì§€ ì•Šì„ ê²½ìš°, terminalì—ì„œ ë¡œê·¸ì¸ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "!huggingface-cli login --token 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\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e070b2dca9704216917b1d570fb7952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"\n",
    "ì§€ë¬¸:\n",
    "{paragraph}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ì„ íƒì§€:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.\n",
    "ì •ë‹µ:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"\n",
    "ì§€ë¬¸:\n",
    "{paragraph}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "<ë³´ê¸°>:\n",
    "{question_plus}\n",
    "\n",
    "ì„ íƒì§€:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.\n",
    "ì •ë‹µ:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <ë³´ê¸°>ê°€ ìˆì„ ë•Œ\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <ë³´ê¸°>ê°€ ì—†ì„ ë•Œ\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ì§€ë¬¸ì„ ì½ê³  ë‹µì„ ë§í•´ì£¼ì„¸ìš”.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e6113710374bd6b98b6a863ec602d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# ë°ì´í„° í† í°í™”\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]ì§€ë¬¸ì„ ì½ê³  ë‹µì„ ë§í•´ì£¼ì„¸ìš”.[|endofturn|]\n",
      "[|user|]\n",
      "ì§€ë¬¸:\n",
      "ë‹¤ìŒë‹¬ ì´ˆê¹Œì§€ ì„œìš¸ ë“± ì „êµ­ì˜ ë‚® ìµœê³ ê¸°ì˜¨ì´ 20ë„ë¥¼ ë„˜ëŠ” ë•Œ ì´ë¥¸ ì´ˆì—¬ë¦„ ë‚ ì”¨ê°€ ê³„ì†ë  ì „ë§ì´ë‹¤. ê¸°ìƒì²­ì€ â€œì´ë™ì„± ê³ ê¸°ì••ì˜ ì˜í–¥ìœ¼ë¡œ 16ì¼ê¹Œì§€ ì „êµ­ì— ë§‘ì€ ë‚ ì”¨ê°€ ì´ì–´ì§€ê² ë‹¤â€ë©° â€œì „êµ­ì˜ ë‚® ìµœê³ ê¸°ì˜¨ì´ 20ë„ë¥¼ ë„˜ëŠ” ê³ ì˜¨ í˜„ìƒì´ ê³„ì†ë  ê²ƒâ€ì´ë¼ê³  13ì¼ ì˜ˆë³´í–ˆë‹¤. ê¸°ìƒì²­ì€ 14ì¼ ì„œìš¸ì˜ ë‚® ìµœê³ ê¸°ì˜¨ì´ ìµœê·¼ 30ë…„ë˜ í‰ë…„ì¹˜(17.3ë„)ë¥¼ ì›ƒë„ëŠ” 23ë„ë¥¼ ê¸°ë¡í•˜ê³ , ëŒ€êµ¬ì™€ ì°½ì›ì˜ ê²½ìš° í‰ë…„ì¹˜(19~20ë„)ë¥¼ ì›ƒë„ëŠ” 27ë„ê¹Œì§€ ì˜¤ë¥´ê² ë‹¤ê³  ë°í˜”ë‹¤.ëª©ìš”ì¼ì¸ 17ì¼ë¶€í„°ëŠ” ì„œìª½ì—ì„œ ë‹¤ê°€ì˜¤ëŠ” ê¸°ì••ê³¨ì˜ ì˜í–¥ìœ¼ë¡œ ì „êµ­ì— ë¹„ê°€ ë‚´ë¦¬ë©´ì„œ ê³ ì˜¨ í˜„ìƒì´ ì ì‹œ ì£¼ì¶¤í•  ì „ë§ì´ë‹¤. ë‹¤ë§Œ 18ì¼ ë¹„ê°€ ê·¸ì¹œ ë’¤ ì£¼ë§ë¶€í„°ëŠ” ë˜ë‹¤ì‹œ ê³ ì˜¨ í˜„ìƒì´ ì´ì–´ì§„ë‹¤.ê¸°ìƒì²­ì€ ì´ë‹¬ í•˜ìˆœì—ëŠ” ë‚¨ìª½ìœ¼ë¡œë¶€í„° ë”°ëœ»í•œ ê¸°ë¥˜ê°€ ìœ ì…ë˜ë©´ì„œ ê¸°ì˜¨ì´ í¬ê²Œ ì˜¤ë¥¼ ë•Œê°€ ìˆê² ë‹¤ê³  ë‚´ë‹¤ë´¤ë‹¤. ë‚¨ë¶€ ì§€ì—­ì€ ê³³ì— ë”°ë¼ ë‚® ìµœê³ ê¸°ì˜¨ì´ 30ë„ê¹Œì§€ ì¹˜ì†Ÿì„ ì „ë§ì´ë‹¤. ê¸°ìƒì²­ì€ ë‹¤ìŒë‹¬ ì´ˆìˆœì—ë„ ì´ë™ì„± ê³ ê¸°ì••ì˜ ì˜í–¥ì„ ë°›ì•„ ë§‘ê³  ê±´ì¡°í•œ ë‚ ì´ ì´ì–´ì§€ë©´ì„œ í‰ë…„ì— ë¹„í•´ ë†’ì€ ê¸°ì˜¨ì„ ê¸°ë¡í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸:\n",
      "ê¸°ìƒì²­ì´ ì˜ˆë³´í•œ ì„œìš¸ì˜ ë‚® ìµœê³ ê¸°ì˜¨ì€ ìµœê·¼ 30ë…„ë˜ í‰ë…„ì¹˜ë³´ë‹¤ ëª‡ ë„ ë†’ì€ê°€?\n",
      "\n",
      "ì„ íƒì§€:\n",
      "1 - 20ë„\n",
      "2 - 21ë„\n",
      "3 - 22ë„\n",
      "4 - 23ë„\n",
      "5 - 24ë„\n",
      "\n",
      "1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.\n",
      "ì •ë‹µ:\n",
      "[|assistant|]4[|endofturn|]\n",
      "\n",
      "[|system|]ì§€ë¬¸ì„ ì½ê³  ë‹µì„ ë§í•´ì£¼ì„¸ìš”.[|endofturn|]\n",
      "[|user|]\n",
      "ì§€ë¬¸:\n",
      "\"ë‚˜ëŠ” ìš°ë¦¬ì™€ ìš°ë¦¬ ì¡°ìƒì´ ì˜ì ì¸ ë¬¸ì œì—ì„œ í–‰ë™í•˜ë©° ì†ì„¸ì˜ ê¶Œë ¥ì„ í–‰ì‚¬í•˜ëŠ” í¬ê³  ë†€ë¼ìš´ ì‹¤ìˆ˜ë¥¼ ëª©ê²©í•©ë‹ˆë‹¤. ì£¼ ì˜ˆìˆ˜ì˜ ë§ˆì§€ë§‰ ìœ ì–¸ì¥ì„ ì—¬ëŸ¬ ë²ˆ ì½ì—ˆìœ¼ë‚˜ ì˜ˆìˆ˜ê»˜ì„œ ê¸°êº¼ì´ ì†ì„¸ì˜ ë©´ë¥˜ê´€ê³¼ ì •ë¶€ë¥¼ ë°›ì•„ë“¤ì´ì…¨ë”ë¼ë©´ ì˜ˆìˆ˜ì˜ ì˜ì ì¸ ì‚¬ì•ˆê³¼ ì™•êµ­ì˜ ë¬¸ì œì— ì†ì„¸ë‚˜ ì‹œë¯¼ ê¶Œë ¥ì˜ ì•„ì£¼ ì‘ì€ ë¶€ë¶„ì´ë¼ë„ ì œì•ˆí•˜ì…¨ì„ ê²ƒì¸ë°, ì´ëŠ” ìœ ì–¸ì—ì„œ ë‹¨ í•œ ë§ˆë””ë„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‹œë¯¼ êµ­ê°€ê°€ ì¸ê°„ì˜ ì˜í˜¼ì— ì¢…êµ, ì˜ˆë°°, ì‚¬ì—­, (ì¢…êµ ë° ì‹œë¯¼ ë¬¸ì œì—ì„œ) ë§¹ì„¸, ì‹­ì¼ì¡°, ì‹œê¸°, ë‚ , ê²°í˜¼, ì„±ì†Œ ë§¤ì¥ ë“±ì„ ê°•ìš”í•˜ëŠ” ê²ƒì€ ì• ì„í•˜ê²Œë„ ê·¸ë¦¬ìŠ¤ë„ ì˜ˆìˆ˜ì— ëŒ€í•œ ê°„ì¦ì— ì–´ê¸‹ë‚˜ëŠ” ì¼ì´ ì•„ë‹ ìˆ˜ ì—†ìŠµë‹ˆë‹¤...\" ë¡œì € ìœŒë¦¬ì—„ìŠ¤(Roger Williams), ê·¸ë¦¬ìŠ¤ë„ê»˜ ì†í•˜ì§€ ì•ŠëŠ” ìƒì—…ì  ì‚¬ì—­, 1652\n",
      "\n",
      "ì§ˆë¬¸:\n",
      "ì²­êµë„ë“¤ì´ ì¢…êµë¥¼ ì‹¤ì²œí•  ììœ ê°€ í™•ëŒ€ë˜ì–´ì•¼ í•œë‹¤ê³  ë¯¿ì—ˆë˜ ì‚¬ëŒë“¤ì„ ê³ ë¥´ì‹­ì‹œì˜¤.\n",
      "\n",
      "ì„ íƒì§€:\n",
      "1 - ì²­êµë„ë§Œ\n",
      "2 - ëª¨ë“  ê°œì‹ êµë„ë§Œ\n",
      "3 - ëª¨ë“  ê¸°ë…êµì¸ë§Œ\n",
      "4 - ëª¨ë“  ìœ ëŒ€ì¸ê³¼ ê¸°ë…êµì¸ë§Œ\n",
      "\n",
      "1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.\n",
      "ì •ë‹µ:\n",
      "[|assistant|]1[|endofturn|]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "dataset_indices = list(range(len(tokenized_dataset)))\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "train_index, test_index = train_test_split(\n",
    "    dataset_indices, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.select(train_index)\n",
    "eval_dataset = tokenized_dataset.select(test_index)\n",
    "\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(train_dataset[1][\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion ë¶€ë¶„ë§Œ í•™ìŠµí•˜ê¸° ìœ„í•œ data collator ì„¤ì •\n",
    "\n",
    "- í…ìŠ¤íŠ¸ ì¤‘ response_template ê¹Œì§€ëŠ” ignore_index ë¡œ loss ê³„ì‚°ì—ì„œ ì œì™¸\n",
    "- í…ìŠ¤íŠ¸ ì¤‘ response_template ì´í›„ëŠ” í•™ìŠµì— í¬í•¨ (ì •ë‹µ + eos í† í°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"[|assistant|]\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ logits ë¥¼ ì¡°ì •í•˜ì—¬ ì •ë‹µ í† í° ë¶€ë¶„ë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric ë¡œë“œ\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# ì •ë‹µ í† í° ë§¤í•‘\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# metric ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "\n",
    "    # í† í°í™”ëœ ë ˆì´ë¸” ë””ì½”ë”©\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = list(map(lambda x: x.split(\"[|endofturn|]\")[0].strip(), labels))\n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸íŠ¸ ë³€í™˜\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[BOS]',\n",
       " 'eos_token': '[|endofturn|]',\n",
       " 'unk_token': '[UNK]',\n",
       " 'pad_token': '[|endofturn|]'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad token ì„¤ì •\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:43\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    573\u001b[0m ):\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:846\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 846\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "dataset_indices = list(range(len(tokenized_dataset)))\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "train_index, test_index = train_test_split(\n",
    "    dataset_indices, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.select(train_index)\n",
    "eval_dataset = tokenized_dataset.select(test_index)    \n",
    "\n",
    "\n",
    "current_output_dir = \"outputs_exaone\"\n",
    "    \n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    max_seq_length=1024,\n",
    "    output_dir=current_output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1.5e-5,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from accelerate import Accelerator\n",
    "import torch, gc\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # ì˜ˆ: ì´ˆê¸° ë ˆì´ì–´ë¥¼ ë™ê²°\n",
    "        param.requires_grad = False\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc2966cc13d44c1838d8b494d33eb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO í•™ìŠµëœ Checkpoint ê²½ë¡œ ì…ë ¥\n",
    "checkpoint_path = \"../../data/outputs_exaone/checkpoint-1218\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model = model.to('cuda')  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data ê²½ë¡œ ì…ë ¥\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <ë³´ê¸°>ê°€ ìˆì„ ë•Œ\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <ë³´ê¸°>ê°€ ì—†ì„ ë•Œ\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ì§€ë¬¸ì„ ì½ê³ , í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•œ ë’¤ ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•´ì„œ ë‹µì„ êµ¬í•˜ì„¸ìš”.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2\"\n",
    "\n",
    "# # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # ì˜ˆ: ì´ˆê¸° ë ˆì´ì–´ë¥¼ ë™ê²°\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "<timed exec>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 869/869 [20:47<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 51s, sys: 5min 56s, total: 20min 47s\n",
      "Wall time: 20min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "\n",
    "        outputs = model(\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        row = {\"id\": _id, \"answer\": predict_value}\n",
    "                \n",
    "        target_logit_list = [logit.item() for logit in target_logit_list]\n",
    "        if len_choices < len(pred_choices_map):\n",
    "            target_logit_list += [None] * (len(pred_choices_map) - len_choices)\n",
    "        for i, logit in enumerate(target_logit_list):\n",
    "            row[f\"logit_{pred_choices_map[i]}\"] = logit\n",
    "\n",
    "        infer_results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "\n",
    "        outputs = model(\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output_exaone_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
