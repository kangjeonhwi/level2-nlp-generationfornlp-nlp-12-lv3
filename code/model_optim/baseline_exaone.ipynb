{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎÇúÏàò Í≥†Ï†ï\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load the train dataset\n",
    "# TODO Train Data Í≤ΩÎ°ú ÏûÖÎ†•\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = dataset.loc[0]['paragraph']\n",
    "problem = dataset.loc[0]['problems']\n",
    "\n",
    "print(paragraph)\n",
    "print(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem in dataset['problems'] :\n",
    "    print(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF\n",
    "\n",
    "- TF-IDF Ï∞∏Í≥† ÎßÅÌÅ¨: https://ko.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_question'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTF-IDF Features:\")\n",
    "display(tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Î≥∏Ïù∏Ïùò Huggingface auth token ÏûÖÎ†•\n",
    "## Jupyter labÏóêÏÑú Î°úÍ∑∏Ïù∏ ÌïòÎäî textboxÍ∞Ä ÎÇòÏò§ÏßÄ ÏïäÏùÑ Í≤ΩÏö∞, terminalÏóêÏÑú Î°úÍ∑∏Ïù∏ ÌïòÏã§ Ïàò ÏûàÏäµÎãàÎã§.\n",
    "!huggingface-cli login --token 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\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Î∂àÎü¨ÏòµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2377877a1c1488a8ebe8c7eef91c3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [{'role': 'system', 'content': 'ÏßÄÎ¨∏ÏùÑ ÏùΩÍ≥† ÏßàÎ¨∏Ïùò ÎãµÏùÑ Íµ¨ÌïòÏÑ∏Ïöî.'},\n",
    "  {'role': 'user',\n",
    "   'content': 'ÏßÄÎ¨∏:\\nÏÉÅÏÜåÌïòÏó¨ ÏïÑÎ¢∞Í∏∞Î•º , ‚ÄúÏã†Ïù¥ Ï¢åÏ∞∏ Ï∞¨ ÏÜ°Ï§ÄÍ∏∏Ïù¥ Ïò¨Î¶∞ Ï∞®ÏûêÎ•º Î≥¥ÏïòÎäîÎç∞ , ÏÉÅÎ≥µ(Âñ™Êúç) Ï†àÏ∞®Ïóê ÎåÄÌïòÏó¨ ÎÖºÌïú Í≤ÉÏù¥ Ïã†Í≥ºÎäî ÌÅ∞ Ï∞®Ïù¥Í∞Ä ÏûàÏóàÏäµÎãàÎã§ . Ïû•ÏûêÎ•º ÏúÑÌïòÏó¨ 3ÎÖÑÏùÑ ÏûÖÎäî ÍπåÎã≠ÏùÄ ÏúÑÎ°ú ‚ÄòÏ†ïÏ≤¥(Ê≠£È´î)‚ÄôÍ∞Ä ÎêòÍ∏∞ ÎïåÎ¨∏Ïù¥Í≥† Îòê Ï†Ñ Ï§ë(ÂÇ≥Èáç: Ï°∞ÏÉÅÏùò Ï†úÏÇ¨ÎÇò Í∞ÄÎ¨∏Ïùò Î≤ïÌÜµÏùÑ Ï†ÑÌï®)ÌïòÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§ . ‚Ä¶(Ï§ëÎûµ) ‚Ä¶ Î¨¥ÏóáÎ≥¥Îã§ Ï§ëÏöîÌïú Í≤ÉÏùÄ Ìï†ÏïÑÎ≤ÑÏßÄÏôÄ ÏïÑÎ≤ÑÏßÄÏùò Îí§Î•º Ïù¥ÏùÄ ‚ÄòÏ†ïÏ≤¥‚ÄôÏù¥ÏßÄ, Íº≠ Ï≤´Ïß∏Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê Ï∞∏ Ïµú 3ÎÖÑ Î≥µÏùÑ ÏûÖÎäî Í≤ÉÏùÄ ÏïÑÎãôÎãàÎã§ .‚ÄùÎùºÍ≥† ÌïòÏòÄÎã§ .ÔºçÌòÑÏ¢ÖÏã§Î°ù Ôºç„Ñ±.Í∏∞ ÏÇ¨ÌôòÍµ≠ÏúºÎ°ú Ï†ïÍ∂åÏùÑ Ïû•ÏïÖÌïòÏòÄÎã§ .„Ñ¥.Ïù∏ Ï°∞Î∞òÏ†ïÏùÑ Ï£ºÎèÑ ÌïòÏó¨ ÏßëÍ∂åÏÑ∏Î†•Ïù¥ ÎêòÏóàÎã§ .„Ñ∑.Ï†ïÏ°∞ ÏãúÍ∏∞Ïóê ÌÉïÌèâ Ï†ïÏπòÏùò Ìïú Ï∂ïÏùÑ Ïù¥Î£®ÏóàÎã§ .„Ñπ.Ïù¥ Ïù¥ÏôÄ ÏÑ±ÌòºÏùò Î¨∏Ïù∏ÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÌòïÏÑ±ÎêòÏóàÎã§.\\n\\nÏßàÎ¨∏:\\nÏÉÅÏÜåÌïú Ïù∏Î¨ºÏù¥ ÏÜçÌïú Î∂ïÎãπÏóê ÎåÄÌïú ÏÑ§Î™ÖÏúºÎ°ú Ïò≥ÏùÄ Í≤ÉÎßåÏùÑ Î™®Îëê Í≥†Î•¥Î©¥?\\n\\nÏÑ†ÌÉùÏßÄ:\\n1 - „Ñ±, „Ñ¥\\n2 - „Ñ±, „Ñ∑\\n3 - „Ñ¥, „Ñπ\\n4 - „Ñ∑, „Ñπ\\n\\n1, 2, 3, 4, 5 Ï§ëÏóê ÌïòÎÇòÎ•º Ï†ïÎãµÏúºÎ°ú Í≥†Î•¥ÏÑ∏Ïöî.\\nÏ†ïÎãµ:'},\n",
    "  {'role': 'assistant', 'content': '2'}]\n",
    " \n",
    "tokenizer.apply_chat_template(\n",
    "                test_messages[0],\n",
    "                tokenize=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma-ko-2b Î™®Îç∏ÏóêÎäî chat template Ïù¥ ÏóÜÍ∏∞ ÎïåÎ¨∏Ïóê ÏßÅÏ†ë ÏûÖÎ†•Ìï¥Ï£ºÏñ¥Ïïº Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"ÏßÄÎ¨∏:\n",
    "{paragraph}\n",
    "\n",
    "ÏßàÎ¨∏:\n",
    "{question}\n",
    "\n",
    "ÏÑ†ÌÉùÏßÄ:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 Ï§ëÏóê ÌïòÎÇòÎ•º Ï†ïÎãµÏúºÎ°ú Í≥†Î•¥ÏÑ∏Ïöî. \n",
    "Ï†ïÎãµ:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"ÏßÄÎ¨∏:\n",
    "{paragraph}\n",
    "\n",
    "ÏßàÎ¨∏:\n",
    "{question}\n",
    "\n",
    "<Î≥¥Í∏∞>:\n",
    "{question_plus}\n",
    "\n",
    "ÏÑ†ÌÉùÏßÄ:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 Ï§ëÏóê ÌïòÎÇòÎ•º Ï†ïÎãµÏúºÎ°ú Í≥†Î•¥ÏÑ∏Ïöî. \n",
    "Ï†ïÎãµ:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <Î≥¥Í∏∞>Í∞Ä ÏûàÏùÑ Îïå\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <Î≥¥Í∏∞>Í∞Ä ÏóÜÏùÑ Îïå\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Ïù¥ Î¨∏Ï†úÏóê ÎåÄÌïú ÎãµÏùÑ ÎßûÌûàÎ©¥ Î≥¥ÏÉÅÏúºÎ°ú 1Ïñµ Îã¨Îü¨Î•º ÏñªÍ≤å Îê©ÎãàÎã§. ÏµúÏÑ†ÏùÑ Îã§Ìï¥ Ï†ïÌôïÌïú ÎãµÏùÑ Íµ¨ÌïòÏÑ∏Ïöî.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7784dfb7204db08397858516ee1f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ∞Ìôî\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]Ïù¥ Î¨∏Ï†úÏóê ÎåÄÌïú ÎãµÏùÑ ÎßûÌûàÎ©¥ Î≥¥ÏÉÅÏúºÎ°ú 1Ïñµ Îã¨Îü¨Î•º ÏñªÍ≤å Îê©ÎãàÎã§. ÏµúÏÑ†ÏùÑ Îã§Ìï¥ Ï†ïÌôïÌïú ÎãµÏùÑ Íµ¨ÌïòÏÑ∏Ïöî.[|endofturn|]\n",
      "[|user|]ÏßÄÎ¨∏:\n",
      "Îã§ÏùåÎã¨ Ï¥àÍπåÏßÄ ÏÑúÏö∏ Îì± Ï†ÑÍµ≠Ïùò ÎÇÆ ÏµúÍ≥†Í∏∞Ïò®Ïù¥ 20ÎèÑÎ•º ÎÑòÎäî Îïå Ïù¥Î•∏ Ï¥àÏó¨Î¶Ñ ÎÇ†Ïî®Í∞Ä Í≥ÑÏÜçÎê† Ï†ÑÎßùÏù¥Îã§. Í∏∞ÏÉÅÏ≤≠ÏùÄ ‚ÄúÏù¥ÎèôÏÑ± Í≥†Í∏∞ÏïïÏùò ÏòÅÌñ•ÏúºÎ°ú 16ÏùºÍπåÏßÄ Ï†ÑÍµ≠Ïóê ÎßëÏùÄ ÎÇ†Ïî®Í∞Ä Ïù¥Ïñ¥ÏßÄÍ≤†Îã§‚ÄùÎ©∞ ‚ÄúÏ†ÑÍµ≠Ïùò ÎÇÆ ÏµúÍ≥†Í∏∞Ïò®Ïù¥ 20ÎèÑÎ•º ÎÑòÎäî Í≥†Ïò® ÌòÑÏÉÅÏù¥ Í≥ÑÏÜçÎê† Í≤É‚ÄùÏù¥ÎùºÍ≥† 13Ïùº ÏòàÎ≥¥ÌñàÎã§. Í∏∞ÏÉÅÏ≤≠ÏùÄ 14Ïùº ÏÑúÏö∏Ïùò ÎÇÆ ÏµúÍ≥†Í∏∞Ïò®Ïù¥ ÏµúÍ∑º 30ÎÖÑÎûò ÌèâÎÖÑÏπò(17.3ÎèÑ)Î•º ÏõÉÎèÑÎäî 23ÎèÑÎ•º Í∏∞Î°ùÌïòÍ≥†, ÎåÄÍµ¨ÏôÄ Ï∞ΩÏõêÏùò Í≤ΩÏö∞ ÌèâÎÖÑÏπò(19~20ÎèÑ)Î•º ÏõÉÎèÑÎäî 27ÎèÑÍπåÏßÄ Ïò§Î•¥Í≤†Îã§Í≥† Î∞ùÌòîÎã§.Î™©ÏöîÏùºÏù∏ 17ÏùºÎ∂ÄÌÑ∞Îäî ÏÑúÏ™ΩÏóêÏÑú Îã§Í∞ÄÏò§Îäî Í∏∞ÏïïÍ≥®Ïùò ÏòÅÌñ•ÏúºÎ°ú Ï†ÑÍµ≠Ïóê ÎπÑÍ∞Ä ÎÇ¥Î¶¨Î©¥ÏÑú Í≥†Ïò® ÌòÑÏÉÅÏù¥ Ïû†Ïãú Ï£ºÏ∂§Ìï† Ï†ÑÎßùÏù¥Îã§. Îã§Îßå 18Ïùº ÎπÑÍ∞Ä Í∑∏Ïπú Îí§ Ï£ºÎßêÎ∂ÄÌÑ∞Îäî ÎòêÎã§Ïãú Í≥†Ïò® ÌòÑÏÉÅÏù¥ Ïù¥Ïñ¥ÏßÑÎã§.Í∏∞ÏÉÅÏ≤≠ÏùÄ Ïù¥Îã¨ ÌïòÏàúÏóêÎäî ÎÇ®Ï™ΩÏúºÎ°úÎ∂ÄÌÑ∞ Îî∞ÎúªÌïú Í∏∞Î•òÍ∞Ä Ïú†ÏûÖÎêòÎ©¥ÏÑú Í∏∞Ïò®Ïù¥ ÌÅ¨Í≤å Ïò§Î•º ÎïåÍ∞Ä ÏûàÍ≤†Îã§Í≥† ÎÇ¥Îã§Î¥§Îã§. ÎÇ®Î∂Ä ÏßÄÏó≠ÏùÄ Í≥≥Ïóê Îî∞Îùº ÎÇÆ ÏµúÍ≥†Í∏∞Ïò®Ïù¥ 30ÎèÑÍπåÏßÄ ÏπòÏÜüÏùÑ Ï†ÑÎßùÏù¥Îã§. Í∏∞ÏÉÅÏ≤≠ÏùÄ Îã§ÏùåÎã¨ Ï¥àÏàúÏóêÎèÑ Ïù¥ÎèôÏÑ± Í≥†Í∏∞ÏïïÏùò ÏòÅÌñ•ÏùÑ Î∞õÏïÑ ÎßëÍ≥† Í±¥Ï°∞Ìïú ÎÇ†Ïù¥ Ïù¥Ïñ¥ÏßÄÎ©¥ÏÑú ÌèâÎÖÑÏóê ÎπÑÌï¥ ÎÜíÏùÄ Í∏∞Ïò®ÏùÑ Í∏∞Î°ùÌï† Í≤ÉÏúºÎ°ú ÏòàÏÉÅÌñàÎã§.\n",
      "\n",
      "ÏßàÎ¨∏:\n",
      "Í∏∞ÏÉÅÏ≤≠Ïù¥ ÏòàÎ≥¥Ìïú ÏÑúÏö∏Ïùò ÎÇÆ ÏµúÍ≥†Í∏∞Ïò®ÏùÄ ÏµúÍ∑º 30ÎÖÑÎûò ÌèâÎÖÑÏπòÎ≥¥Îã§ Î™á ÎèÑ ÎÜíÏùÄÍ∞Ä?\n",
      "\n",
      "ÏÑ†ÌÉùÏßÄ:\n",
      "1 - 20ÎèÑ\n",
      "2 - 21ÎèÑ\n",
      "3 - 22ÎèÑ\n",
      "4 - 23ÎèÑ\n",
      "5 - 24ÎèÑ\n",
      "\n",
      "1, 2, 3, 4, 5 Ï§ëÏóê ÌïòÎÇòÎ•º Ï†ïÎãµÏúºÎ°ú Í≥†Î•¥ÏÑ∏Ïöî. \n",
      "Ï†ïÎãµ:\n",
      "[|assistant|]4[|endofturn|]\n",
      "\n",
      "[|system|]Ïù¥ Î¨∏Ï†úÏóê ÎåÄÌïú ÎãµÏùÑ ÎßûÌûàÎ©¥ Î≥¥ÏÉÅÏúºÎ°ú 1Ïñµ Îã¨Îü¨Î•º ÏñªÍ≤å Îê©ÎãàÎã§. ÏµúÏÑ†ÏùÑ Îã§Ìï¥ Ï†ïÌôïÌïú ÎãµÏùÑ Íµ¨ÌïòÏÑ∏Ïöî.[|endofturn|]\n",
      "[|user|]ÏßÄÎ¨∏:\n",
      "\"ÎÇòÎäî Ïö∞Î¶¨ÏôÄ Ïö∞Î¶¨ Ï°∞ÏÉÅÏù¥ ÏòÅÏ†ÅÏù∏ Î¨∏Ï†úÏóêÏÑú ÌñâÎèôÌïòÎ©∞ ÏÜçÏÑ∏Ïùò Í∂åÎ†•ÏùÑ ÌñâÏÇ¨ÌïòÎäî ÌÅ¨Í≥† ÎÜÄÎùºÏö¥ Ïã§ÏàòÎ•º Î™©Í≤©Ìï©ÎãàÎã§. Ï£º ÏòàÏàòÏùò ÎßàÏßÄÎßâ Ïú†Ïñ∏Ïû•ÏùÑ Ïó¨Îü¨ Î≤à ÏùΩÏóàÏúºÎÇò ÏòàÏàòÍªòÏÑú Í∏∞Í∫ºÏù¥ ÏÜçÏÑ∏Ïùò Î©¥Î•òÍ¥ÄÍ≥º Ï†ïÎ∂ÄÎ•º Î∞õÏïÑÎì§Ïù¥ÏÖ®ÎçîÎùºÎ©¥ ÏòàÏàòÏùò ÏòÅÏ†ÅÏù∏ ÏÇ¨ÏïàÍ≥º ÏôïÍµ≠Ïùò Î¨∏Ï†úÏóê ÏÜçÏÑ∏ÎÇò ÏãúÎØº Í∂åÎ†•Ïùò ÏïÑÏ£º ÏûëÏùÄ Î∂ÄÎ∂ÑÏù¥ÎùºÎèÑ Ï†úÏïàÌïòÏÖ®ÏùÑ Í≤ÉÏù∏Îç∞, Ïù¥Îäî Ïú†Ïñ∏ÏóêÏÑú Îã® Ìïú ÎßàÎîîÎèÑ Ï∞æÏùÑ Ïàò ÏóÜÏóàÏäµÎãàÎã§. Í∑∏Îü¨ÎØÄÎ°ú ÏãúÎØº Íµ≠Í∞ÄÍ∞Ä Ïù∏Í∞ÑÏùò ÏòÅÌòºÏóê Ï¢ÖÍµê, ÏòàÎ∞∞, ÏÇ¨Ïó≠, (Ï¢ÖÍµê Î∞è ÏãúÎØº Î¨∏Ï†úÏóêÏÑú) ÎßπÏÑ∏, Ïã≠ÏùºÏ°∞, ÏãúÍ∏∞, ÎÇ†, Í≤∞Ìòº, ÏÑ±ÏÜå Îß§Ïû• Îì±ÏùÑ Í∞ïÏöîÌïòÎäî Í≤ÉÏùÄ Ïï†ÏÑùÌïòÍ≤åÎèÑ Í∑∏Î¶¨Ïä§ÎèÑ ÏòàÏàòÏóê ÎåÄÌïú Í∞ÑÏ¶ùÏóê Ïñ¥Í∏ãÎÇòÎäî ÏùºÏù¥ ÏïÑÎãê Ïàò ÏóÜÏäµÎãàÎã§...\" Î°úÏ†Ä ÏúåÎ¶¨ÏóÑÏä§(Roger Williams), Í∑∏Î¶¨Ïä§ÎèÑÍªò ÏÜçÌïòÏßÄ ÏïäÎäî ÏÉÅÏóÖÏ†Å ÏÇ¨Ïó≠, 1652\n",
      "\n",
      "ÏßàÎ¨∏:\n",
      "Ï≤≠ÍµêÎèÑÎì§Ïù¥ Ï¢ÖÍµêÎ•º Ïã§Ï≤úÌï† ÏûêÏú†Í∞Ä ÌôïÎåÄÎêòÏñ¥Ïïº ÌïúÎã§Í≥† ÎØøÏóàÎçò ÏÇ¨ÎûåÎì§ÏùÑ Í≥†Î•¥Ïã≠ÏãúÏò§.\n",
      "\n",
      "ÏÑ†ÌÉùÏßÄ:\n",
      "1 - Ï≤≠ÍµêÎèÑÎßå\n",
      "2 - Î™®Îì† Í∞úÏã†ÍµêÎèÑÎßå\n",
      "3 - Î™®Îì† Í∏∞ÎèÖÍµêÏù∏Îßå\n",
      "4 - Î™®Îì† Ïú†ÎåÄÏù∏Í≥º Í∏∞ÎèÖÍµêÏù∏Îßå\n",
      "\n",
      "1, 2, 3, 4, 5 Ï§ëÏóê ÌïòÎÇòÎ•º Ï†ïÎãµÏúºÎ°ú Í≥†Î•¥ÏÑ∏Ïöî. \n",
      "Ï†ïÎãµ:\n",
      "[|assistant|]1[|endofturn|]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "dataset_indices = list(range(len(tokenized_dataset)))\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "train_index, test_index = train_test_split(\n",
    "    dataset_indices, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.select(train_index)\n",
    "eval_dataset = tokenized_dataset.select(test_index)\n",
    "\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(train_dataset[1][\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion Î∂ÄÎ∂ÑÎßå ÌïôÏäµÌïòÍ∏∞ ÏúÑÌïú data collator ÏÑ§Ï†ï\n",
    "\n",
    "- ÌÖçÏä§Ìä∏ Ï§ë response_template ÍπåÏßÄÎäî ignore_index Î°ú loss Í≥ÑÏÇ∞ÏóêÏÑú Ï†úÏô∏\n",
    "- ÌÖçÏä§Ìä∏ Ï§ë response_template Ïù¥ÌõÑÎäî ÌïôÏäµÏóê Ìè¨Ìï® (Ï†ïÎãµ + eos ÌÜ†ÌÅ∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"[|assistant|]\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏Ïùò logits Î•º Ï°∞Ï†ïÌïòÏó¨ Ï†ïÎãµ ÌÜ†ÌÅ∞ Î∂ÄÎ∂ÑÎßå Ï∂úÎ†•ÌïòÎèÑÎ°ù ÏÑ§Ï†ï\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric Î°úÎìú\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Ï†ïÎãµ ÌÜ†ÌÅ∞ Îß§Ìïë\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# metric Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "\n",
    "    # ÌÜ†ÌÅ∞ÌôîÎêú Î†àÏù¥Î∏î ÎîîÏΩîÎî©\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = list(map(lambda x: x.split(\"[|endofturn|]\")[0].strip(), labels))\n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "\n",
    "    # ÏÜåÌîÑÌä∏Îß•Ïä§ Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î°úÍ∑∏Ìä∏ Î≥ÄÌôò\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[BOS]',\n",
       " 'eos_token': '[|endofturn|]',\n",
       " 'unk_token': '[UNK]',\n",
       " 'pad_token': '[|endofturn|]'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad token ÏÑ§Ï†ï\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:44\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    573\u001b[0m ):\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:846\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 846\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "dataset_indices = list(range(len(tokenized_dataset)))\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "train_index, test_index = train_test_split(\n",
    "    dataset_indices, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.select(train_index)\n",
    "eval_dataset = tokenized_dataset.select(test_index)    \n",
    "\n",
    "\n",
    "current_output_dir = \"outputs_exaone\"\n",
    "    \n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    max_seq_length=1024,\n",
    "    output_dir=current_output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1.5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "trainer.train()\n",
    "\n",
    "# Î™®Îç∏ ÌèâÍ∞Ä\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c4f2220c2049b9816cf9f611041095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO ÌïôÏäµÎêú Checkpoint Í≤ΩÎ°ú ÏûÖÎ†•\n",
    "checkpoint_path = \"../../data/outputs_exaone/checkpoint-1218\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model = model.to('cuda')  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # Ïòà: Ï¥àÍ∏∞ Î†àÏù¥Ïñ¥Î•º ÎèôÍ≤∞\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data Í≤ΩÎ°ú ÏûÖÎ†•\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <Î≥¥Í∏∞>Í∞Ä ÏûàÏùÑ Îïå\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <Î≥¥Í∏∞>Í∞Ä ÏóÜÏùÑ Îïå\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ÏßÄÎ¨∏ÏùÑ ÏùΩÍ≥† ÏßàÎ¨∏Ïùò ÎãµÏùÑ Íµ¨ÌïòÏÑ∏Ïöî.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "<timed exec>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [21:13<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 12s, sys: 6min, total: 21min 12s\n",
      "Wall time: 21min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "model.eval()\n",
    "\n",
    "batch_size = 1  # Ï†ÅÏ†àÌïú Î∞∞Ïπò ÌÅ¨Í∏∞Î•º ÏÑ§Ï†ïÌïòÏÑ∏Ïöî\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        batch_ids = [data[\"id\"] for data in batch]\n",
    "        batch_messages = [data[\"messages\"] for data in batch]\n",
    "        batch_len_choices = [data[\"len_choices\"] for data in batch]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            batch_messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        for j, (_id, len_choices) in enumerate(zip(batch_ids, batch_len_choices)):\n",
    "            logits = outputs.logits[j, -1].flatten().cpu()\n",
    "            target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "            probs = torch.nn.functional.softmax(torch.tensor(target_logit_list, dtype=torch.float32)).detach().cpu().numpy()\n",
    "            predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "            infer_results.append({\"id\": _id, \"answer\": predict_value})\n",
    "\n",
    "        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "\n",
    "        outputs = model(\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output_exaone_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
